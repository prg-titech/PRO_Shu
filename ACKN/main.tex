\documentclass[PRO,english]{ipsj}
\usepackage{PROpresentation}
\PROheadtitle{2020-128-(x): Manuscript for presentation at IPSJ-SIGPRO, 12 3 2020.}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{listings}
\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}



\begin{document}

\title{Improving Keyword-based Code Recommendation by Exploiting Context Information}
\author{Shu Aochi}{Tokyo Institute of Technology}[shuaochi@prg.is.titech.ac.jp]
\author{Hidehiko Masuhara}{Tokyo Institute of Technology}[masuhara@is.titech.ac.jp]

\begin{abstract}
Code recommendation provides code fragments that the programmer likely to type in. One of the advanced code recommendation techniques is keyword programming, which can reflect the programmers' intention. Keyword programming lets the user specify keywords and recommends expressions that contain as many of them. Another one is neural code completion, which uses neural networks to recommend likely occurring expressions according to the context (the program text preceding the cursor position). Previous work showed that the accuracy of a keyword programming system is not high enough. One of the reasons is that the existing keyword programming always recommends shorter expressions without using the context information. In this presentation, we improve keyword programming by combining a neural code completion technique. In addition to the occurrence of keyword, the ranking algorithm incorporates the likeliness factor of the code fragment concerning the context. To estimate the likeliness, we utilize a neural network-based sentence generator. Thus, we can achieve a more complicatedly suitable code fragment and generate a candidate list varying along with different contexts. We implemented our proposal for Java called ACKN as an Eclipse plug-in. The implementation is publicly available.
\end{abstract}

\maketitle

\section{Introduction}
% Not needed for PRO
% Selecting a proper programming environment is essential for a programmer when they start to build a project. A Java programmer is likely to choose Eclipse or IntelliJ, while a C programmer prefers Visual Studio. A principle to judge which environment is appropriate is the efficiency in developing code.

\emph{Code recommendation}, also called code completion, is one of the common features in modern programming editors that presents a list of code fragments to the programmer so that he or she can input the desired code by merely choosing one of them.
% present one typical example so that the readers can understand what
% we are talking about
While a typical implementation presents the names of available methods/functions/variables that match the letters already typed in the editor, 
% and since we are not working on such a simple code completion
% feature, make a remark that there are many variations
there are many variations with respect to the lengths of the presented code fragments (from function names to a few lines of code), and with respect to information used for making recommendations.

Information used for making recommendations can roughly be classified into two kinds, namely \emph{explicit} intention and \emph{implicit} context.  

Explicit intention is the information that the programmer provides to the system when he or she wants to obtain recommendation.  Examples are \emph{prefix letters}, \emph{an abbreviation}, and \emph{keywords}.  When the programmer wants to type \texttt{InputStreamReader}, he or she can type ``\texttt{Inp}'' or ``\texttt{ISR}'' to a prefix-based or abbreviation-based system, respectively, then the system will generate identifiers that start with \texttt{Inp} or that are concatenation of words starting from \texttt{I}, \texttt{S} and \texttt{R}.  These kind of recommendation systems can be found many programming editors, for example Eclipse IDE.

\emph{Keyword programming}, on which we are based on,  uses keywords\footnote{In keyword programmings, the term \emph{keyword} means a query word used for searching, like the one used for web search engines.  It should not be confused with keywords (or reserved words) in the syntax of programming languages.} as the explicit intention~\cite{KeywordProgramming}.  The key idea is, by letting the programmer provide a bit longer explicit intention, to enable the system can recommend longer expressions or statements.  For example, when the programmer wants to input
\begin{quote}
  \texttt{new BufferedReader(\\
    \qquad{}new InputStreamReader(System.in))},   
\end{quote}
he or she can type ``\texttt{buffered reader in}'' so that the system will recommend expressions including the above one.  (In the next section, we explain how keyword programming generates recommendations.)

Implicit context is the information available in the code and the past behaviors of the programmer.  For example, the recommendation system in Eclipse uses the type information of the expressions around the cursor position, and recommends identifiers (class, variable or method  names) that can form an expression with a matching type.

Implicit context can improve the quality of recommendations.  For example, Robbes et al.\  discovered that the type information and code structure are useful to greatly improve a prefix-based recommendation system~\cite{ProgramHistory}.
% Romain Robbes et al.~\cite{ProgramHistory} propose a code recommendation system that also inputs a few characters, but the recommendations are sorted according to the user’s programming history.


%%%%%%%%%%%%%%%% TODO

Han et al.\ proposed a method to improve an abbreviation-based recommendation system by using a hidden Markov model (HMM)~\cite{Sangmok}.  They construct a HMM of a program corpus, and use it for recommending expressions that are more likely to appear in the corpus.

% I DON'T KNOW how Hu et al.'s work can be positioned yet.  %%%%%%%%%%%%%%%%%%%%%%%%%%%

% Moreover, Sangmok Han et al.~\cite{Sangmok} and Sheng Hu et al.~\cite{Sheng} recommend possible code fragments given an abbreviated input. The former uses a Hidden Markov Model to expand the abbreviation to inputs, while the latter uses a Gaussian mixture model. 

In addition, Greg little et al.~\cite{KeywordProgramming} propose an approach that the user types some keywords to search for an expected expression. And after the keyword programming system provides a list of candidate expressions, the user can look through the list and select the expected one. 

Unlike the default code recommendation system on Eclipse, the user only needs to trigger the keyword programming system once to get the expected expression. For example, after type keywords \textbf{buffered reader in} and run the keyword programming system, the expression \texttt{new BufferedReader(InputStreamReader(System.in))} will be shown on the top of the recommendation list.

In contrast, implicit information denotes information that can be exploited from the context. These kinds of systems can be categorized by different statistical models.

For example, TabNine~\cite{TabNine} and Marcel Bruch et al.~\cite{Marcel} takes the whole context into consideration arranges the recommendations by their probabilities. The previous system calculates probabilities by a GPT-2 model and Marcel Bruch et al. utilize an algorithm named Best Matching Neighbors(BMN) based on the K-Nearest Neighbors algorithm. 

On the other hand, neural networks have developed rapidly in the past 20 years and have shown their strong power in many genres. One of them is the text generation, which aims to generate a likely sentence. Those neural networks text generators are divided by using different neural networks.~\cite{NNTG}

In 2003, Bengio et al.~\cite{Bengio} first use a standard neural model that extends the n-gram paradigm with neural networks. 

Then,a recurrent neural network language model~\cite{RNN} is used in text generation because it can process time-series data.

Moreover, a long short term model(LSTM)~\cite{LSTM} is an optimized RNN designed for learning the long term dependency. In contrast to the standard RNN, LSTM has a better performance to learn from more complicated data.
		
In this paper, we propose a code recommendation system that improves the existing keyword programming system by using neural text generation to concern the context of the user’s editing file.

Our goal is to build a code recommendation system that:
\begin{enumerate}
\item Suitable to all programmers including a beginner and an expert.
\item Recommend the expression that satisfies the user’s purpose, especially when the expression is complex.
\end{enumerate}

We implement a plug-in on Eclipse named ACKN based on a context-aware keyword programming technique. 

\section{Background}
\subsection{Keyword Programming}
Keyword programming is a technique that translates a keyword query to an expression. A keyword programming system has four parts, namely input, extraction, generation, and ranking. 

The user first inputs a keyword query in the source code. Then the system collects the information of local variables and methods from the source code and generates all possible expressions by following the syntax and typing rules. Finally, the system calculates the scores of those expressions and shows that have higher scores.
\subsubsection{Input}
The keyword query is similar to the search query of a search engine, which conveys the programmer’s intention. For example, if a programmer wants to write an expression to display the name of a variable \texttt{f} on the monitor, the keyword query would be \textbf{print f name}. If the user wants to get the max number between number a and number b, they could use \textbf{get maximum between a and b}.

The keyword query is the whole tokens in the line where the cursor position is in the active editor. From the information of a cursor position, the system can know which local variable can be referred and which method can be called in that position.

\subsubsection{Extraction}
Keyword programming extracts essential information before generation. The information includes:
\begin{itemize}
\item all available object class name,
\item the name and type of a local variable,
\item the name, type, and object class of a field, and
\item the name, return type, object class, and argument types of a method.
\end{itemize}

For example, supposed the system wants to generate the expression \texttt{System.out.println(result)}. First, it is necessary to know the information of a class name \texttt{System}. And a field \texttt{out} that is in the object class \textit{System} and return the type \textit{PrintStream}. Also, a method named \texttt{println} where the receiver type is \textit{PrintStream}, the return type is \textit{void} and the argument type is \textit{String}. Finally, a local variable \texttt{result} that returns the type \textit{String}.

\subsubsection{Generation}
After obtaining those elements from a source file, the system generates all possible expressions that obey the syntax and type rules of the programming language. For instance, considering the program

\begin{lstlisting}[language=Java]
package example;

import java.io.BufferedReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class Example{
	public static void main(String[] args){
	    String result = ``result'';
	}

	public List getLines(BufferedReader src) throws IOException{
	    List array = new ArrayList();
	    while(src.ready()){
	        |
	    }

	    return null;
	}
}
\end{lstlisting}

The cursor is in line 16. In this line, we can call the method and field in the class \textit{BufferedReader}, \textit{IOException}, \textit{ArrayList} and \textit{List}. Moreover, we also can refer the variable \texttt{src} and \texttt{array}. Therefore, a method invocation such as \texttt{array.add(src.readLine())} is available, where \texttt{add} is one of the methods declared in the class \textit{List}, and \texttt{readLine} is a method declared in the class \textit{BufferedReader}. 

However, an expression like \texttt{System.out.print(result)} would lead to a compile error, because it is not able to refer the variable \texttt{result} in the line 10. Another method invocation such as \texttt{result.print()} is also forbidden since it does not follow the type rules.


\subsubsection{Ranking}
\label{subsubsection:Ranking}
The code completion system of Eclipse organizes the recommendation alphabetically. For example, the method \texttt{add} shows in front of the method \texttt{concat} on the recommendation list.

In contrast, a keyword programming system shows the recommendations in the order of scores calculated by rules that:
\begin{itemize}
\item -0.05 for each height of abstract syntax tree,
\item +1.0 if the expression contains a token inside the keyword query,
\item -0.01 if the token is not in the keyword query, and
\item +0.001, when the element is a local variable or a member method.
\end{itemize}

Token is obtained by splitting an expression with punctuation and upper case letter. For example, the expression \texttt{str.toUpperCase()} has 4 tokens, which are \texttt{str}, \texttt{to}, \texttt{upper} and \texttt{case}.

We demonstrate the rule by an example of calculating the score for a function \texttt{array.add()} given the keyword \textbf{add} and \textbf{line}.

First, since the height of \texttt{array.add()} is two, the initial score is -0.1.

Then, since the token \texttt{array} is not in the keyword query, the score becomes -0.11.

Because the token \texttt{add} is in the keyword query, the score becomes +0.89.

Finally, since the token array is also a local variable, the final score is +0.891.

\subsection{Neural text generation}
Word2Vec
Mechnism

\section{Problem}
Although keyword programming can help a user to complete the program conveniently, there is still a problem that the expected expression, especially a complicated one, is not able to be shown as the first candidate.

For example, if a user wants to read the standard input information from console, in Java, most of users would write the expression \texttt{new BufferedReader(new InputStreamReader(System.in))}. 

When the programmer uses keyword programming to get this expression, after the user types the keywords \textbf{reader in} and triggers the system, the first candidate would be \texttt{new InputStreamReader(System.in)}, and the 12th candidate is the expected expression \texttt{new BufferedReader(new InputStreamReader(System.in))}. 


% Likewise, if the user wants to display a message on the console, the expression could be \texttt{System.out.println(message)}. Given a keyword query \textbf{println message}, the first candidate is \texttt{System.err.println(message)} and the second one is \texttt{System.out.println(message)}.

In the previous research, an expression is 90\% to be shown on the top of the recommendation list if the keywords use as same tokens as in the expression. For example, given a keyword query \textbf{new buffered reader new input stream reader system in}, the expression \texttt{new BufferedReader(new InputStreamReader(System.in))} would be the first candidate.

However, like other code recommendation technique, keyword programming aims to shorten programming time. Thus, it would be meaningless if the user types all tokens in the expected expression. Alternatively, a keyword programming user would be more likely to choose relatively fewer substrings in the expected expression as the keyword query such as \textbf{reader in}. 

By using the scoring function in the section~\ref{subsubsection:Ranking}, the score of the expression \texttt{new InputStreamReader(System.in)} would be +1.81. On the other hand, the score of the expression \texttt{new BufferedReader(new InputStreamReader}\\\texttt{(System.in))} would be +1.74. Therefore, the former expression is in a higher position in the recommendation list.

The reason is that an expression has a higher score calculated by the scoring function when it is shorter and contains more keywords. Therefore, when the expected expression becomes more complicated and the number of keywords is relatively fewer, it is hard to be shown in a higher position in the recommendation list. In this paper, complicate denotes that the expression has a larger abstract syntax tree or contains many tokens. For instance, the height of \texttt{new BufferedReader(new InputStreamReader(System.in))} is 4, and it contains 9 tokens from \texttt{new} to \texttt{in}.

\section{Proposal: Using a NN text generator}
Our proposal is to use a neural network text generator to recommend more context-dependent expressions.

The approach is straightforward. For each expression, we calculate not only a score by the orginal scoring function but also the occurance probability. We add these two values to represent the final score of the expression.

For example, if the user is editing a program where the cursor is in line 6, and the keyword query is \texttt{read in}:
\begin{lstlisting}[language=Java]
import java.io.BufferedReader;
import java.io.InputStreamReader;

public class ReadInput{
	public static void main(String[] args){
	    reader in|
	}
}
\end{lstlisting}

By using the original scoring function, the score of the expression is +1.74. 

Subsequently, to calculate the occurance probability of the expression \texttt{new BufferedReader(new InputStreamReader(System.in))}, we need 7 steps. 

First, we take the token sequence from \texttt{import} to \texttt{args} as an argument and predict the probability of the token \texttt{new} by the neural network model. 

Then, append the token \texttt{new} to the token sequence and predict the probability of the token \texttt{BufferReader}. Repeat this procedure until get the probability of the last token \texttt{in}. 

At this moment, we have 6 probability numbers for each tokens.  Finally, we add these 6 probabilities and use it to represent the occurance probability of the expression.

Thus, in the new scoring function, the final score for the expression \texttt{new BufferedReader(new InputStreamReader(System.in))} is +(1.74 + \textit{probability}).

By exploiting the implicit information from the previous codes and considering the context, an expression would have a higher probability value if it is more likely to be the next expression. 

These implicit information includes the imported package declaration, the identifier names and the order of previous method invocations. For example, in the preceding program, the program has imported two package \textit{java.io.BufferedReader} and \textit{java.io.InputStreamReader}. Moreover, the class name is \textit{ReadInput}. The user is more likely to write the expression \texttt{new BufferedReader(new InputStreamReader(System.in))} than \texttt{new InputStreamReader(System.in)}. Therefore, the probability of the former expression is higher than the latter one. In other words, the candidate expression \texttt{new BufferedReader(new InputStreamReader(System.in))} could be shown in a higher position.


\section{Implementation}

Beam Search

Word2Vec

LSTM

\section{Evaluation}
\subsection{Procedure}

\subsection{Result}

\subsection{Discussion}

\section{Future Work}
Word2Vec
Speed: NN

\section{Related Work}

\section{Conclusion}




\begin{thebibliography}{10}
\bibitem{ProgramHistory} Robbes R, Lanza M. How program history can improve code completion[C]//2008 23rd IEEE/ACM International Conference on Automated Software Engineering. IEEE, 2008: 317-326.

\bibitem{Sangmok} Han S, Wallace D R, Miller R C. Code completion from abbreviated input[C]//2009 IEEE/ACM International Conference on Automated Software Engineering. IEEE, 2009: 332-343.

\bibitem{Sheng} Hu S, Xiao C, Qin J, et al. Autocompletion for Prefix-Abbreviated Input[C]//Proceedings of the 2019 International Conference on Management of Data. 2019: 211-228.

\bibitem{KeywordProgramming} Little G, Miller R C. Keyword programming in Java[J]. Automated Software Engineering, 2009, 16(1): 37.

\bibitem{TabNine} TabNine: https://tabnine.com/

\bibitem{Marcel} Bruch M, Monperrus M, Mezini M. Learning from examples to improve code completion systems[C]//Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering. 2009: 213-222.

\bibitem{NNTG} Lu S, Zhu Y, Zhang W, et al. Neural text generation: Past, present and beyond[J]. arXiv preprint arXiv:1803.07133, 2018.

\bibitem{Bengio} Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.

\bibitem{RNN} Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network based language model[C]//Eleventh annual conference of the international speech communication association. 2010.


\bibitem{LSTM} Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997, 9(8): 1735-1780.

\end{thebibliography}


\end{document}