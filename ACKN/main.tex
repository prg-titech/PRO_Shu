\documentclass[PRO,english]{ipsj}
\usepackage{PROpresentation}
\PROheadtitle{2019-5-(2): Manuscript for presentation at IPSJ-SIGPRO, 12 3 2020.}
\usepackage [dvipdfmx] {graphicx}
\usepackage{latexsym}

\usepackage{listings}
\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

\newcommand{\secref}[1]{Section~\ref{#1}}

\begin{document}

\title{Improving Keyword-based Code Recommendation by Exploiting Context Information}
\author{Shu Aochi}{Tokyo Institute of Technology}[shuaochi@prg.is.titech.ac.jp]
\author{Hidehiko Masuhara}{Tokyo Institute of Technology}[masuhara@is.titech.ac.jp]

\begin{abstract}
Code recommendation provides code fragments that the programmer likely to type in. One of the advanced code recommendation techniques is keyword programming, which can reflect the programmers' intention. Keyword programming lets the user specify keywords and recommends expressions that contain as many of them. Another one is neural code completion, which uses neural networks to recommend likely occurring expressions according to the context (the program text preceding the cursor position). Previous work showed that the accuracy of a keyword programming system is not high enough. One of the reasons is that the existing keyword programming always recommends shorter expressions without using the context information. In this presentation, we improve keyword programming by combining a neural code completion technique. In addition to the occurrence of keyword, the ranking algorithm incorporates the likeliness factor of the code fragment concerning the context. To estimate the likeliness, we utilize a neural network-based sentence generator. Thus, we can achieve a more complicatedly suitable code fragment and generate a candidate list varying along with different contexts. We implemented our proposal for Java called ACKN as an Eclipse plug-in. The implementation is publicly available.
\end{abstract}

\maketitle

\section{Introduction}
% Not needed for PRO
% Selecting a proper programming environment is essential for a programmer when they start to build a project. A Java programmer is likely to choose Eclipse or IntelliJ, while a C programmer prefers Visual Studio. A principle to judge which environment is appropriate is the efficiency in developing code.

\emph{Code recommendation}, also called code completion, is one of the common features in modern programming editors that presents a list of code fragments to the programmer so that he or she can input the desired code by merely choosing one of them.
% present one typical example so that the readers can understand what
% we are talking about
While a typical implementation presents the names of available methods/functions/variables that match the letters already typed in the editor, 
% and since we are not working on such a simple code completion
% feature, make a remark that there are many variations
there are many variations with respect to the lengths of the presented code fragments (from function names to a few lines of code), and with respect to information used for making recommendations.

Information used for making recommendations can roughly be classified into two kinds, namely \emph{explicit} intention and \emph{implicit} context.  
% TODO: they are not exclusive, rather used in combination.

Explicit intention is the information that the programmer provides to the system when he or she wants to obtain recommendation.  Examples are \emph{prefix letters}, \emph{an abbreviation}, and \emph{keywords}.  When the programmer wants to type \texttt{BufferedReader}, he or she can type ``\texttt{Buf}'' or ``\texttt{BR}'' to a prefix-based or abbreviation-based system, respectively, then the system will generate identifiers that start with \texttt{Buf} or that are concatenation of words starting from \texttt{B} and \texttt{R}.  These kind of recommendation systems can be found many programming editors, for example Eclipse IDE.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.38]{Figure/Eclipse.pdf}
\caption{Eclipse code completion system}
\label{fig:Eclipse}
\end{figure}


\emph{Keyword programming}, on which we are based on,  uses keywords\footnote{In keyword programmings, the term \emph{keyword} means a query word used for searching, like the one used for web search engines.  It should not be confused with keywords (or reserved words) in the syntax of programming languages.} as the explicit intention~\cite{little2009keyword}.  The key idea is, by letting the programmer provide a bit longer explicit intention, to enable the system to recommend longer expressions or statements.  For example, when the programmer wants to input
\begin{quote}
  \texttt{new BufferedReader(\\
    \qquad{}new InputStreamReader(System.in))},   
\end{quote}
he or she can type ``\texttt{buffered reader in}'' so that the system will recommend expressions including the above one.  (In the next section, we explain how keyword programming generates recommendations.)

Implicit context is the information available in the code and the past behaviors of the programmer.  For example, the recommendation system in Eclipse uses the type information of the expressions around the cursor position, and recommends identifiers (class, variable or method  names) that can form an expression with a matching type.

Implicit context can improve the quality of recommendations.  For example, Robbes et al.\  discovered that the type information and code structure are useful to greatly improve a prefix-based recommendation system~\cite{robbes2008program}.
% Romain Robbes et al.~\cite{ProgramHistory} propose a code recommendation system that also inputs a few characters, but the recommendations are sorted according to the user’s programming history.

Han et al.\ proposed a method to improve an abbreviation-based recommendation system by using a hidden Markov model (HMM)~\cite{han2009code}.  They construct a HMM of a program corpus, and use it for recommending expressions that are more likely to appear in the corpus.

% I DON'T KNOW how Hu et al.'s work can be positioned yet.  %%%%%%%%%%%%%%%%%%%%%%%%%%%

% Moreover, Sangmok Han et al.~\cite{Sangmok} and Sheng Hu et al.~\cite{Sheng} recommend possible code fragments given an abbreviated input. The former uses a Hidden Markov Model to expand the abbreviation to inputs, while the latter uses a Gaussian mixture model. 


% TODO: knowledge from a corpus 

% In contrast, implicit information denotes information that can be exploited from the context. These kinds of systems can be categorized by different statistical models.

Many recommendation systems use implicit context information in combination with knowledge from a corpus.  In other words, those systems recommend expressions/identifiers that ``programmers who wrote these also wrote.'' 
For example, TabNine~\cite{deep-tabnine2019} and Bruch et al.'s work~\cite{bruch2009learning} extract a sequence of tokens or a sequence of method calls before the cursor position in the editor, and recommend expressions or identifiers that frequently appear in the expressions that share the same sequence in the corpus.  %  takes the whole context into consideration arranges the recommendations by their probabilities. The previous system calculates probabilities by a GPT-2 model and Marcel Bruch et al. utilize an algorithm named Best Matching Neighbors(BMN) based on the K-Nearest Neighbors algorithm. 

% https://tabnine.com/blog/deep/

% @Misc{deep-tabnine2019,
%   author =     {{TabNine, Inc.}},
%   title =      {Autocompletion with deep learning},
%   howpublished = {https://tabnine.com/blog/deep/},
%   month =      {July 15},
%   year =   2019,
%   note =   {Accessed February 11, 2020.}}

% GPT-2
% @article{radford2019language,
%   title={Language models are unsupervised multitask learners},
%   author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
%   journal={OpenAI Blog},
%   volume={1},
%   number={8},
%   pages={9},
%   year={2019}
% }

Those system exploit corpora by using statistical methods in order to cope with large corpora.  Bruch et al.\ use a clustering method, for example.  TabNine uses a deep learning method~\cite{deep-tabnine2019} called GPT-2~\cite{radford2019language}.  Our work also uses a deep learning method for augmenting the keyword programming with the corpus knowledge.  We will explain a fundamental mechanism of using a deep learning method for exploiting a corpus in the next section.

%%%%%%%%%%%%%%%% Deleted as they will be explained in Section 2.

% On the other hand, neural networks have developed rapidly in the past 20 years and have shown their strong power in many genres. One of them is the text generation, which aims to generate a likely sentence. Those neural networks text generators are divided by using different neural networks.~\cite{NNTG}

% In 2003, Bengio et al.~\cite{Bengio} first use a standard neural model that extends the n-gram paradigm with neural networks. 

% Then,a recurrent neural network language model~\cite{RNN} is used in text generation because it can process time-series data.

% Moreover, a long short term model(LSTM)~\cite{LSTM} is an optimized RNN designed for learning the long term dependency. In contrast to the standard RNN, LSTM has a better performance to learn from more complicated data.

This paper proposes a method of improving the keyword programming by exploiting program corpus knowledge.  With the support from a neural network based sentence generator, it tries to recommend expressions not just containing the programmer provided keywords, but those more likely appear in the corpus.  By doing so, we aim at making the keyword programming usable for larger expressions.  

The rest of the paper is organized as follows.  We first introduce how the original keyword programming recommends expressions based on the generate-and-ranking method.  We also overview what a neural network can generate sentences with a corpus (\secref{sec:background}).  We then illustrate that the original keyword programming works poorly for recommending larger expressions (\secref{sec:problem}).  We present our proposal, which lets the keyword programming recommend expressions that more likely appear in the corpus, based on the probabilities from the sentence generator (\secref{sec:proposal}).  We implemented the proposal as an Eclipse plug-in called ACKN (\secref{sec:implementation}) and evaluated the proposal by performing an experiment of inserting 15 expressions with the original and proposed keyword programming (\secref{sec:evaluation}). 

% Our goal is to build a code recommendation system that:
% \begin{enumerate}
% \item Suitable to all programmers including a beginner and an expert.
% \item Recommend the expression that satisfies the user’s purpose, especially when the expression is complex.
% \end{enumerate}

% We implement a plug-in on Eclipse named ACKN based on a context-aware keyword programming technique. 

\section{Background}\label{sec:background}
% TODO: in introduction, define terms like "expression-level recommendation" and "identifier-level recommendation"
\subsection{Keyword Programming}
The keyword programming~\cite{little2009keyword} is an expression-level recommendation system based on keywords.  It is used when the programmer wants to write an expression (\emph{the desired expression})\footnote{The keyword programming system can recommend not only expressions but also one or multiple statements.  Here, we only explain the case for expressions for simplicity.} in a partially written program (\emph{the program context}), by typing a set of words (\emph{the keywords}) into the position where it should appear (\emph{the cursor position}).  The system then shows an ordered list of expressions (\emph{the recommendations}), one of which will be selected by the programmer and inserted into the cursor position. Below, we explain the mechanism in the four steps, namely reading keywords, extracting context information, expression generation, and ranking. 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.38]{Figure/KeywordProgrammingExample.pdf}
\caption{Code completion by using keyword programming}
\label{fig:KP}
\end{figure}

% The user first inputs a keyword query in the source code. Then the system collects the information of local variables and methods from the source code and generates all possible expressions by following the syntax and typing rules. Finally, the system calculates the scores of those expressions and shows that have higher scores.
\subsubsection{Keywords}
%As explained in the previous section, the keyword programming requires \emph{keywords} as an explicit input from the programmer.

The keywords are set of words separated by a space character, and should be part of the desired expression.  Similar to the keyword-based web search engines, the user of the keyword programming needs to select the words that represent the desired expression well.

Even when the programmer does not exactly know the desired expression, it is possible to provide keywords by using the terms at the semantic level.  For example, when the programmer wants to display the file name of the variable \texttt{f} of the type \texttt{File}, providing ``\texttt{print f name}'' can lead to a recommendation \texttt{System.out.print(f.getName())} without knowing the exact method name.

% If the user wants to get the max number between number a and number b, they could use \textbf{get maximum between a and b}.

\subsubsection{Extracting Context Information}
As program context, the system extracts information of
\begin{itemize}
\item the variables and their types that can be referenced at the cursor position, and 
\item the types (including method and field signatures) available in the program files.
\end{itemize}
The information will be used for generating expressions that have valid types in the next step.

% Keyword programming extracts essential information before generation. The information includes:
% \begin{itemize}
% \item all available object class name,
% \item the name and type of a local variable,
% \item the name, type, and object class of a field, and
% \item the name, return type, object class, and argument types of a method.
% \end{itemize}

% For example, supposed the system wants to generate the expression \texttt{System.out.println(result)}. First, it is necessary to know the information of a class name \texttt{System}. And a field \texttt{out} that is in the object class \textit{System} and return the type \textit{PrintStream}. Also, a method named \texttt{println} where the receiver type is \textit{PrintStream}, the return type is \textit{void} and the argument type is \textit{String}. Finally, a local variable \texttt{result} that returns the type \textit{String}.

\subsubsection{Expression Generation}
The system then generates all valid expressions and passes them to the next ranking step.  A ``valid'' expression is such an expression that will be safely compiled when it is inserted at the cursor position.  Since there can be infinitely many valid expressions, an implementation actually generates only expressions with a limited size, and avoids generating expressions that will have lower scores in the next step.

% After obtaining those elements from a source file, the system generates all possible expressions that obey the syntax and type rules of the programming language. For instance, considering the program

%%%%%%%%% TODO: show this example at the beginning of section 2.1

% \begin{lstlisting}[language=Java]
% package example;

% import java.io.BufferedReader;
% import java.io.IOException;
% import java.util.ArrayList;
% import java.util.List;

% public class Example{
%     public static void main(String[] args){
%         String result = ``result'';
%     }

%     public List getLines(BufferedReader src) throws IOException{
%         List array = new ArrayList();
%         while(src.ready()){
%             |
%         }

%         return null;
%     }
% }
% \end{lstlisting}

% The cursor is in line 16. In this line, we can call the method and field in the class \textit{BufferedReader}, \textit{IOException}, \textit{ArrayList} and \textit{List}. Moreover, we also can refer the variable \texttt{src} and \texttt{array}. Therefore, a method invocation such as \texttt{array.add(src.readLine())} is available, where \texttt{add} is one of the methods declared in the class \textit{List}, and \texttt{readLine} is a method declared in the class \textit{BufferedReader}. 

% However, an expression like \texttt{System.out.print(result)} would lead to a compile error, because it is not able to refer the variable \texttt{result} in the line 10. Another method invocation such as \texttt{result.print()} is also forbidden since it does not follow the type rules.

\subsubsection{Ranking}
\label{subsubsection:Ranking}

The system calculates scores of the generated expressions in order to show top $N$ recommendations.  The scoring function prefers expressions that are more concise and have more keywords.  The function is defined in this formula:
\begin{equation}
  - 0.05 N + 1.0 K  -0.01 D + 0.001 L \label{eq:scoring}
\end{equation}
where
\begin{description}
\item[$N$] is the nesting level of the expression (i.e., the height of its abstract syntax tree representation),
\item[$K$] is the number of keywords that match tokens in the expression,
\item[$D$] is the number of tokens in the expression that do not match any keyword, and
\item[$L$] is the number of tokens that reference local variables or method names.
\end{description}
Here, a token is a component of an identifier split by the ``camel case.''  For example, the method name \texttt{getName} consists of the two tokens namely \texttt{get} and \texttt{name}; hence the keyword \texttt{name} is considered as appeared therein.

% The code completion system of Eclipse organizes the recommendation alphabetically. For example, the method \texttt{add} shows in front of the method \texttt{concat} on the recommendation list.

%In contrast, a keyword programming system shows the recommendations in the order of scores calculated by rules that:
% \begin{itemize}
% \item -0.05 for each height of abstract syntax tree,
% \item +1.0 if the expression contains a token inside the keyword query,
% \item -0.01 if the token is not in the keyword query, and
% \item +0.001, when the element is a local variable or a member method.
% \end{itemize}

% Token is obtained by splitting an expression with punctuation and upper case letter. For example, the expression \texttt{str.toUpperCase()} has 4 tokens, which are \texttt{str}, \texttt{to}, \texttt{upper} and \texttt{case}.

For example, the score of \texttt{array.add()} (where \texttt{array} is a local variable) with respect to the keywords \texttt{add} and \texttt{line} is calculated in this way.  The above four parameters are determined as follows:
\begin{itemize}
\item $N=2$
\item $K=1$ (for \texttt{add})
\item $D=1$ (for \texttt{array}), and
\item $L=1$ (for \texttt{array}).
\end{itemize}
Thus, the score is 
$$
  - 0.05 \cdot 2 + 1.0 \cdot 1  -0.01 \cdot 1 + 0.001 \cdot 1 = 0.891.
$$

\subsubsection{Beam search}
It is a search problem to select expressions with the highest score among all possible generations. In the previous research, they used dynamic programming to generate one result by finding the local optimal solution and \emph{A*} (acturally \emph{beam search}) to get multiple generations.

Beam search is a heuristic graph search algorithm. It is based on breadth first search with width constraint. In each depth, it first sorts the candidates by a scoring function. Then remove the node with lower score and continue to search on the remaining node. \emph{bw}, short for beam width, denotes the number of remaining nodes for each depth.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.23]{Figure/BeamSearch.pdf}
\caption{Beam search in expression generation}
\label{fig:BeamSearch}
\end{figure}

For example, in Fig~\ref{fig:BeamSearch}, suppose the keywords are ``\textbf{print out result}'', and the expressions in the third black block stand for all possible expressions that the depth of AST is under 3. If the user sets the \emph{bw} to be 2, then the system only remain two expressions before generate a deeper one. The remaining expressions are shown in the red block.


\subsection{Neural network text generation}

Our proposal in this paper uses a neural network text generation technique for improving the keyword programming.  Since we use the technique by merely retargeting the domain from natural language sentences to programs, we simple overview of its functionality here.

Neural network text generation is a technique, which is originally developed in the domain of natural language processing, that can train a neural network by using a large corpus of text, so that it will generate, given an input sequence of words, a sequence of words that likely to follow the input sequence.  

%  can be accomplished in 4 steps. We demonstrate this by an example of predicting the next word given a sentence in Shakespeare's style. 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.3]{Figure/NNTG.pdf}
\caption{Using nerual network text generation to get a text in Shakespear's style}
\label{fig:NNTG}
\end{figure}

\figref{fig:NNTG} illustrates the three main operations in neural network text generation, namely \emph{word embedding,} \emph{training}, and \emph{predicting}.

\emph{Word embedding} operation converts between a word in the text and a numerical vector that is used as input and output of the generator network.  Among many embedding techniques, we use an embedding method called \emph{Word2Vec}~\cite{mikolov2013distributed}, which can naturally embed semantic similarity into the vector representation.  Note that Word2Vec is based on neural network technology, we need to train another network by using a corpus.
Though there are also more sophisticated and domain specific embedding methods (for example embedding nodes in a tree structure~\cite{hidehikoasttoken2vec}), we do not consider in this paper.  

\emph{Training} a neural network is to adjust parameters in the network by providing training data set from the corpus.  A training data for text generation is a sequence of words appear in the corpus as an input, and a word that appear just after the sequence as an expected output.  For example, when there is a phrase ``Over hill, over dale'' in the corpus, there will be three training data, namely
$\langle[\mbox{``\texttt{over}''}], \mbox{``\texttt{hill}''}\rangle$, 
$\langle[\mbox{``\texttt{over}''}, \mbox{``\texttt{hill}''}], \mbox{``\texttt{over}''}\rangle$,  and
$\langle[\mbox{``\texttt{over}''}, \mbox{``\texttt{hill}''},\mbox{``\texttt{over}''}], \mbox{``\texttt{dale}''}\rangle$.

In our work, we use the LSTM (long short-term memory) network~\cite{hochreiter1997long} for text generation.  LSTM is one of the recurrent neural network (RNN) models.  RNN is one of the network models for handling inputs in a form of a sequence by retaining a state inside of the network.  LSTM is developed for handling sequences that have structures (such as natural language grammars and phrase structures) by separately maintaining the states affected by immediately preceding words and by distantly preceding words.

% Second, the system builds 2 lists that are used as the inputs of the neural networks. One list contains a word sequence, and the other one contains the next word. For example, the sentence \textit{"Over hill, over dale"} can create 3 pairs of data. The first sequence is \textit{over} and the next word is \textit{hill}. The second sequence is \textit{over hill} and the next word is \textit{over}. The last sequence is \textit{over hill over} and the corresponding next word is \textit{dale}. After obtaining the data, we need to map each word to a numeric value by using word embedding.

% Third, input the data and train the neural networks with it. 

The trained network can \emph{predict} the next word for a given input sequence of words.  Precisely, the output of the network is probability distribution of multiple words.  We can therefore determine the word with the highest probability, or a ranked list of words with higher probabilities.

The network can also be used for predicting a sequence of words followed by the input sequence by providing the next predicted word as the next input word.

% Finally, predict the probability of the next word given a word sequence. For example, given a sentence \textit{ake thee of} then the word \textit{thy} would have the highest probability to be the next word calculated by the neural networks model.

% Moreover, if the user wants to generate a sentence, they need to iterate the last procedure after append the word with the highest probability to the end of the previous word sequence. To be more specific, the probability of the next word is calculated by the neural model given the word sequence \textit{ake thee of thy}.


\section{Problems}\label{sec:problem}

The original keyword programming does not always work well.  We here point out two problematic cases, namely preference of shorter expressions and ignorance of context, and discuss the causes of the problems.

\subsection{Preference of shorter expressions}
Though it is difficult to evaluate the effectiveness of this kind of recommendation systems, the original paper~\cite{little2009keyword} reported that the accuracy drops when the desired expressions get larger, even with an experiment with artificially prepared keywords.  We also confirmed that the quality of recommendation degrades when we tried to input a larger expressions with our re-implementation of the original keyword programming system.

% Although keyword programming can help a user to complete the program conveniently, there is still a problem that the expected expression, especially a complicated one, is not able to be shown as the first candidate.

We can understand the cause of the problem from the scoring function (\ref{eq:scoring}) as it gives a penalty every token that does not match any keyword.  It therefore ranks shorter expressions higher, if containing the same number of keywords.  

We believe that this preference can be more problematic in a practical situation, for example when the programmer wants to input a long idiomatic expression.  Let us see the problem by an example when we want to input the following expression that frequently appear in many programs that reads the standard input on a per-line basis.
\begin{quote}
  \texttt{new BufferedReader(\\
    \qquad new InputStreamReader(System.in))}
\end{quote}
Assume we provided \texttt{reader} and \texttt{in} as the keywords.  Then the system ranks this expression lower with score of $1.74$ than its sub-expression:
\begin{quote}
  \texttt{new InputStreamReader(System.in)}
\end{quote}
with score of $1.81$ because the latter has a fewer number of tokens in total.  By considering the fact that the latter expression alone is used less frequently, it is not ideal.  

%For example, if a user wants to read the standard input information from console, in Java, most of users would write the expression \texttt{new BufferedReader(new InputStreamReader(System.in))}. 

% When the programmer uses keyword programming to get this expression, after the user types the keywords \textbf{reader in} and triggers the system, the first candidate would be \texttt{new InputStreamReader(System.in)}, and the 12th candidate is the expected expression \texttt{new BufferedReader(new InputStreamReader(System.in))}. 



%In the previous research, an expression is 90\% to be shown on the top of the recommendation list if the keywords use as same tokens as in the expression. For example, given a keyword query \textbf{new buffered reader new input stream reader system in}, the expression \texttt{new BufferedReader(new InputStreamReader(System.in))} would be the first candidate.

%However, like other code recommendation technique, keyword programming aims to shorten programming time. Thus, it would be meaningless if the user types all tokens in the expected expression. Alternatively, a keyword programming user would be more likely to choose relatively fewer substrings in the expected expression as the keyword query such as \textbf{reader in}. 

%By using the scoring function in the section~\ref{subsubsection:Ranking}, the score of the expression \texttt{new InputStreamReader(System.in)} would be +1.81. On the other hand, the score of the expression \texttt{new BufferedReader(new InputStreamReader}\\\texttt{(System.in))} would be +1.74. Therefore, the former expression is in a higher position in the recommendation list.

%The reason is that an expression has a higher score calculated by the scoring function when it is shorter and contains more keywords. Therefore, when the expected expression becomes more complicated and the number of keywords is relatively fewer, it is hard to be shown in a higher position in the recommendation list. In this paper, complicate denotes that the expression has a larger abstract syntax tree or contains many tokens. For instance, the height of \texttt{new BufferedReader(new InputStreamReader(System.in))} is 4, and it contains 9 tokens from \texttt{new} to \texttt{in}.

\subsection{Ignorance of context}

Even if we had improved the system to give higher scores to more frequently used expressions, another problem would remain: ignorance of context.  Assume we already typed in \texttt{new BufferedReader()}, and provided \texttt{reader} and \texttt{in} as the keywords in order insert \texttt{new InputStreamReader(System.in)} as the parameter position.  Since the original algorithm calculates the scores regardless the cursor position, it would then give higher score for the expression \texttt{new BufferedReader(new InputStream$\ldots$)} as the parameter of \texttt{new BufferedReader( )}.

\subsection{Summary of the problems}

To summarize, we would like to improve the original keyword programming on the following two respects.
\begin{itemize}
\item It generally gives lower ranks to larger expressions.  However, it also should rank frequently appearing expressions higher even if they are large.  % For example, \texttt{new BufferedReader(new InputStreamReader(System.in))} should have a higher rank over \texttt{new InputStreamReader(System.in)}.
\item It gives the same ranking regardless the context.  However, it should rank expressions higher if the expressions frequently appear in the context at the cursor position.
\end{itemize}

\section{Proposal: Using a NN text generator}\label{sec:proposal}
Our proposal is to use a neural network text generator to recommend more context-dependent expressions.

The approach is straightforward. For each expression, we calculate not only a score by the orginal scoring function but also the occurance probability. We add these two values to represent the final score of the expression.

For example, if the user is editing a program where the cursor is in line 6, and the keyword query is \texttt{read in}:
\begin{lstlisting}[language=Java]
import java.io.BufferedReader;
import java.io.InputStreamReader;

public class ReadInput{
    public static void main(String[] args){
        reader in|
    }
}
\end{lstlisting}

By using the original scoring function, the score of the expression is +1.74. 

Subsequently, to calculate the occurance probability of the expression \texttt{new BufferedReader(new InputStreamReader(System.in))}, we need 3 steps. 

First, we take the token sequence from \texttt{import} to \texttt{args} as an argument and predict the probability of the token \texttt{new} by the neural network model. 

Then, append the token \texttt{new} to the token sequence and predict the probability of the token \texttt{BufferReader}. Repeat this procedure until get the probability of the last token \texttt{in}. 

At this moment, we have 6 probability numbers for each tokens.  Finally, we add these 6 probabilities and use it to represent the occurance probability of the expression.

Thus, in the new scoring function, the final score for the expression \texttt{new BufferedReader(new InputStreamReader(System.in))} is +(1.74 + \textit{probability}).

By exploiting the implicit information from the previous codes and considering the context, an expression would have a higher probability value if it is more likely to be the next expression. 

These implicit information includes the imported package declaration, the identifier names and the order of previous method invocations. For example, in the preceding program, the program has imported two classes \textit{java.io.BufferedReader} and \textit{java.io.InputStreamReader}. Moreover, the class name is \textit{ReadInput}. The user is more likely to write the expression \texttt{new BufferedReader(new InputStreamReader(System.in))} than \texttt{new InputStreamReader(System.in)}. Therefore, the probability of the former expression is higher than the latter one. In other words, the candidate expression \texttt{new BufferedReader(new InputStreamReader(System.in))} could be shown in a higher position.

\begin{lstlisting}[language=Java]
import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.io.FileReader;

public class Read{
    public static void main(String[] args){
        String path = "foo.in";
        reader file|
    }
}
\end{lstlisting}
 
 Neural network text generation and keyword programming are complementary, that is why we conbine these two technique. 

 Neural network text generation could distinguish which expression is likely to be used afterwards. For example, if the program imports classes \textit{java.io.BufferedReader}, \textit{java.io.InputStreamReader} and \textit{java.io.FileReader}. Both \texttt{new BufferedReader(new InputStreamReader(System.in))} and \texttt{new BufferedReader(new FileReader(path))} have a higher probability to be the next expression.

 Then keyword programming can represent the user's intention. For example, if they want to read the input from the specified file, the keyword query would be \textbf{reader file}. Then the candidate \texttt{new BufferedReader(new FileReader(path))} is more likely to be shown in the recommenedation list. On the other hand, if the user wants to read the input from the console, then the keyword query becomes \textbf{reader in}. As a result, \texttt{new BufferedReader(new InputStreamReader(System.in))} would be shown in the toppest of the recommendation list.

\section{Implementation}\label{sec:implementation}

\begin{figure*}[!ht]
\centering
\includegraphics[scale=0.3]{Figure/Overview.pdf}
\caption{Overview of ACKN mechanism}
\label{fig:overview}
\end{figure*}

We build an Eclipse plug-in to implement our idea. The plug-in is written in Java. We named this plug-in as ACKN, which shorts for \textbf{A}uto \textbf{C}ompletion with \textbf{K}eyword programming and \textbf{N}eural network text generation.

\subsection{Search expression from all generations}
In our proposal, we also use beam search in expression
generation. However, we change the standard of the limitation. 

Except for remaining the expressions with higher scores, the system also remaining the expressions with larger probability. We define \emph{pn} as the number of the remaining expressions compared by the probability. 

To the generations in a certain depth, we first sort these by the original scoring function and remain $\emph{bw} - \emph{pn}$ expressions. Then sort other expressions with the new scoring function we introduced in the section~\ref{sec:proposal} and remain \emph{pn} expressions with a higher score.

\subsection{Preprocessing the training data}
\label{subsection:Preprocessing}
Subsequently, we need 2 steps before learning from the neural networks. One is to reduce some unnecessary information from the training program, and another is to transform the code into numbers.

First, we eliminate the comment of each program and transform all string literal to \textit{``stringliteral''}. Since we want to focus on the information from methods and variables, we ignore these noise.

After this step, we use word2vec to map each token to a vector value. We implemented the word2vec by using the gensim package.

Subsequently, we use LSTM as the neural networks model of the neural network text generator. We implement the LSTM networks by using the keras deep learning package.

\section{Evaluation}\label{sec:evaluation}
\subsection{Procedure}
We evaluate ACKN by following five steps. 

First, we prepare 15 expressions decided by ourselves. 

Then, for each expression, we search on the github and collect 21 programs containing the expression in the former step.

After we get the programs, we add 20 of 21 programs for each to the training dataset of the LSTM model. In order to avoid overfitting, we also add 5000 Java programs from the dataset built to evaluate the implementation in the ~\cite{allamanis2015suggesting}, which are the top active Java GitHub projects on January 22nd 2015.

Subsequently, we create a task for each by using the remaining one program. We erase the code of the expression in that program. and make it to be a program with a hole. And for each expression, we think a keyword query that can describe the expression. 

Finally, we run each task on both ACKN and the original keyword programming system. Then, compare the position where the expected expression is in each recommendation list. The recommendation list only show the first 30 results.

\subsection{Result}
Table~\ref{Table1} shows the expected expression and corresponding keyword. The first six expressions using methods or fields related to \textit{System} class. From 7th to 10th are common expressions to process a string. The 11th and 12th are only used in a special task. The 13th and 14th are the expressions that can read a standard input from console. The last expressions has a larger abstract syntax tree.

\begin{table}[!ht]
\centering
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\scalebox{0.9}{
\begin{tabular}{|l|l|l|}
\hline
No.& Keyword Query& Expected Expression\\
\hline
1& \textbf{print result}& \textit{System.out.println(result)}\\
\hline
2& \textbf{print error}& \textit{System.err.println(error)}\\
\hline
3& \textbf{print result}&\textit{System.out.print(result)}\\
\hline
4& \textbf{print error}&\textit{System.err.print(error)}\\
\hline
5& \textbf{get time}&\textit{System.nanotime()}\\
\hline
6& \textbf{get time}&\textit{System.currentTimeMillis()}\\
\hline
7 & \textbf{str at index}&\textit{str.charAt(index)}\\
\hline
8 & \textbf{upcase str}&\textit{str.toUpperCase()}\\
\hline
9 & \textbf{lower str}&\textit{str.toLowerCase()}\\
\hline
10 & \textbf{str from begin to end}&\textit{str.substring(begin, end)}\\
\hline
11 & \textbf{limit capacity to min}&\textit{sb.ensureCapacity(min)}\\
\hline
12 & \textbf{get address of host}&\textit{InetAddress.getLocalHost()}\\
\hline
13 & \textbf{input}&\textit{new Scanner(System.in)}\\
\hline
14 & \textbf{read standard in}&\tabincell{l}{\textit{new BufferedReader(new }\\ \textit{    InputStreamReader(System.in))}}\\
\hline
15 & \textbf{load resource name}& \tabincell{l}{\textit{Thread.currentThread()}\\\textit{    .getContextClassLoader()}\\\textit{    .getResource(name)}}\\
\hline
\end{tabular}}
\caption{keywords and expected expressions of 15 tasks}
\label{Table1}
\end{table}

Table~\ref{Table2} shows the position of the expected expression in the result and the first result by using the original keyword programming system. Table~\ref{Table3} shows the position and the first result by using ACKN. ``$\times$'' stands for the top-30 results do not contain the expected expression.


\begin{table}[!ht]
\centering
\scalebox{0.9}{
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{tabular}{l|l|l}
\hline
Task& Position &Top expressions\\
\hline
1& $\times$ & \textit{System.err.print(result)}\\
\hline
2& $\times$ & \textit{new PrintStream(error)}\\
\hline
3& 2nd & \textit{System.err.print(result)}\\
\hline
4& 3rd & \textit{new PrintStream(error)}\\
\hline
5& 2nd & \textit{System.getProperties()}\\
\hline
6& 4th & \textit{System.getProperties()}\\
\hline
7& 1st & \textit{str.charAt(index)}\\
\hline
8& 10th & \textit{str}\\
\hline
9& 1st & \textit{str.toLowerCase()}\\
\hline
10& 4th  & \textit{str.substring(end, begin).toString( )}\\
\hline
11& $\times$ & \textit{sb.append(min).insert(sb.capacity(),sb.toString())}\\
\hline
12& 2nd  & \textit{local.getHostAddress()}\\
\hline
13& 24th & \textit{new Main().readInputUntilEndOfLine()}\\
\hline
14& 29th & \textit{new InputStreamReader(System.in).read()}\\
\hline
15&$\times$  & \tabincell{l}{\textit{ClassLoader.getSystemClassLoader()}\\ \textit{    .loadClass(ClassLoader} \\ \textit{    .getSystemResource(name).getRef())}}\\
\hline
\end{tabular}}
\caption{position and top result by using the original keyword programming}
\label{Table2}
\end{table}

\begin{table}[!ht]
\centering
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\scalebox{0.9}{
\begin{tabular}{l|l|l}
\hline
Task& Position &Top expressions\\
\hline
1& 5th & \textit{System.err.print(result)}\\
\hline
2& $\times$ & \textit{new PrintStream(error).println()}\\
\hline
3& 2nd & \textit{System.err.print(result)}\\
\hline
4& $\times$ & \textit{new PrintStream(error)}\\
\hline
5& 1st & \textit{System.nanoTime()}\\
\hline
6& 1st & \textit{System.currentTimeMillis()}\\
\hline
7& 1st & \textit{str.charAt(index)}\\
\hline
8& 9th & \textit{str}\\
\hline
9& 1st & \textit{str.toLowerCase()}\\
\hline
10& $\times$  & \textit{str.substring(end, begin).toString( )}\\
\hline
11& 30th & \textit{sb.append(min).insert(sb.capacity(),sb.toString())}\\
\hline
12& 25th  & \textit{new Main().getLocalHost().isMulticastAddress()}\\
\hline
13& 22th & \textit{new Main().readInputUntilEndOfLine()}\\
\hline
14& $\times$ & \textit{new InputStreamReader(System.in,input.readLine())}\\
\hline
15&$\times$  & \tabincell{l}{\textit{ClassLoader.getSystemClassLoader()}\\ \textit{    .loadClass(ClassLoader} \\ \textit{    .getSystemResource(name).getRef())}}\\
\hline
\end{tabular}}
\caption{position and top result by using the AKCN}
\label{Table3}
\end{table}

% \subsection{Discussion}

By using the existing keyword programming,11 tasks can show the expected expression on the list and the expected expression are in the Top-5 results in 8 tasks. In contrast, by using ACKN, 10 tasks can show on the recommendation list, and 6 tasks are in the Top-5 results.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.23]{Figure/ACKN.pdf}
\caption{Code completion by using ACKN}
\label{fig:ACKN}
\end{figure}

\section{Future Work}

\subsection{Support synonyms in the keyword query}
A keyword programming user would use similar words to the substrings of the expected expression in the keyword query. For example, if the user wants to print a file's name on the console, the expression would be \texttt{System.out.print(filename)}. Instead of using \textbf{print filename} as the keyword query, the user would type \textbf{display path}. For a human being, the meaning between two keyword querys are basically identical, whereas it is different for a machine. 

One approach is to use word2vec. As we introduced in the section~\ref{subsection:Preprocessing}, variable names such as \texttt{filename} and \texttt{path} can be recognized as synonym when the surrounding codes are similar. 

However, the word \texttt{display} and \texttt{print} can not be counted as similar with this approach. Because in the programming world, similarity stands for similar function, not similar meaning in the natural language. For instance, \texttt{print} and \texttt{println} can be considered as similar tokens. Because they are often written after the code fragment \texttt{System.out}.

\subsection{Shorten completion time}

In ACKN, we have to calculate the probability for each generation. Thus, it always costs few minutes to get the result and show it on the monitor, which are not acceptable for a code recommendation system. 

\subsection{Using different neural networks}
Except LSTM, there are also many neural networks that can be used in text generation. For example, generative pre-training 2(GPT-2)~\cite{radford2019language}, gated recurrent unit(GRU)~\cite{cho2014learning}, or generative adversarial nets(GANs)~\cite{goodfellow2014generative}. In addition, the LSTM model can be improved by using a attention mechanism. 

\subsection{Adding more training data}
Although we use nearly 5000 program as our training data, it is still not enough for a deep learning issue. Therefore, it is neccessary to add more training data to avoid overfitting.

\section{Conclusion}

In this paper, we propose a code recommendation system based on context-aware keyword programming. We exploit corpora by using an LSTM token generator to predict the probability of the next token and make the keyword programming recommend expressions regarding the probability. We built a Eclipse plug-in and name it ACKN to implement our idea. We evaluated our proposal by comparing the accuracy and precision of inserting 15 expressions. 


\bibliographystyle{unsrt}
\bibliography{ref}



\end{document}